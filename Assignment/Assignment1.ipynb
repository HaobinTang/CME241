{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Assignment 1 by Haobin Tang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Write out the MP/MRP definitions and MRP Value Function definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markov process: \n",
    "\n",
    "A Markov process is a stochastic process that satis es the Markov property. \n",
    "We also recall the notion of Markov property from the last lecture. Consider a stochastic process $(s0, s1, s2, . . . )$ evolving according to some transition dynamics. We say that the stochastic process has the Markov property if and only if $P(si|s0,...,si−1) = P(si|si−1), ∀ i = 1,2,..., $i.e. the transition probability of the next state conditioned on the history including the current state is equal to the transition probability of the next state conditioned only on the current state. In such a scenario, the current state is a su cient statistic of history of the stochastic process, and we say that  the future is independent of the past given present.\n",
    "\n",
    "MRP:\n",
    "\n",
    "A Markov reward process is a Markov process, together with the speci cation of a reward function\n",
    "and a discount factor. It is formally represented using the tuple $(S, P, R, γ)$ which are listed below:\n",
    "\n",
    "• S : A  nite state space.\n",
    "\n",
    "• P : A transition probability model that speci es $P(s′|s)$.\n",
    "\n",
    "• R : A reward function that maps states to rewards (real numbers), i.e $R : S → R$.\n",
    "\n",
    "• γ: Discount factor between 0 and 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MRP Value Function definition:\n",
    "\n",
    "The state value function $V_t(s)$ for a Markov reward process and a state $s ∈ S$ is de ned as the expected return starting from state $s$ at time $t$, and is given by the following expression:\n",
    "$$V_t(s) = E[G_t|s_t = s]$$\n",
    "\n",
    "For $G_t$, return of a Markov reward process is defined as the discounted sum of rewards starting at time $t$ up to the horizon $H$, and is given by the following mathematical formula:\n",
    "\n",
    "$$G_t = \\sum_{i=t}^{H-1}γ^{i−t}r_i , ∀0≤t≤H−1$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Think about the data structures/class design to represent MP/MRP and implement them with clear type declarations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data structures should be:\n",
    "state (S): if it is discrete type=int;\n",
    "       if it is continuous type=float\n",
    "       \n",
    "Action (A): if it is discrete type=int;\n",
    "       if it is continuous type=float\n",
    "       \n",
    "Return (R): type=float\n",
    "\n",
    "Probability (P): type=float\n",
    "\n",
    "Discount factor (γ): type=float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from typing import Mapping, Set, Tuple, Sequence, Any, Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Specifically the data structure/code design of MRP should be incremental (and not independent) to that of MP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MP_to_MRP(MP_Data,Reward):\n",
    "    dic={}\n",
    "    for key in MP_Data:\n",
    "        if Reward.get(key):\n",
    "            dic[key]=(MP_Data[key],Reward[key])\n",
    "        else:\n",
    "            dic[key]=MP_Data[key]\n",
    "    for key in Reward:\n",
    "        if MP_Data.get(key):\n",
    "            pass\n",
    "        else:\n",
    "            dic[key]=Reward[key]\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': ({1: 0.1, 2: 0.6, 3: 0.1, 4: 0.2}, 1),\n",
       " 2: ({1: 0.25, 2: 0.22, 3: 0.24, 4: 0.29}, 1),\n",
       " 3: ({1: 0.7, 2: 0.3, 3: 0, 4: 0}, 1),\n",
       " 4: ({1: 0.3, 2: 0.5, 3: 0.2, 4: 0}, -2)}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MP_transistion_Data={\n",
    "        1: {1: 0.1, 2: 0.6, 3: 0.1, 4: 0.2},\n",
    "        2: {1: 0.25, 2: 0.22, 3: 0.24, 4: 0.29},\n",
    "        3: {1: 0.7, 2: 0.3, 3:0, 4:0},\n",
    "        4: {1: 0.3, 2: 0.5, 3: 0.2, 4:0}\n",
    "    }\n",
    "\n",
    "Reward={1:1,2:1,3:1,4:-2}\n",
    "\n",
    "MP_to_MRP(MP_transistion_Data,Reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. implement the $r(s,s')$ and the $R(s) = \\sum_{s'} p(s,s') * r(s,s')$ definitions of MRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_to_R(r):\n",
    "    dic1={}\n",
    "    dic2={}\n",
    "    for i in r:\n",
    "        dic1[i]={}\n",
    "        for j in r[i]:\n",
    "            dic1[i][j]=r[i][j][0] \n",
    "    for i in r:\n",
    "        dic2[i]=0\n",
    "        for j in r[j]:\n",
    "            dic2[i]=dic2[i]+r[i][j][0]*r[i][j][1]\n",
    "    return MP_to_MRP(dic1,dic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: ({1: 0.1, 2: 0.6, 3: 0.1, 4: 0.2}, 1.2),\n",
       " 2: ({1: 0.25, 2: 0.22, 3: 0.24, 4: 0.29}, 0.735),\n",
       " 3: ({1: 0.7, 2: 0.3, 3: 0, 4: 0}, 1.0),\n",
       " 4: ({1: 0.3, 2: 0.5, 3: 0.2, 4: 0}, 0.9)}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MP_transistion_Data={\n",
    "        1: {1: (0.1,1), 2: (0.6,1), 3: (0.1,1), 4: (0.2,2)},\n",
    "        2: {1: (0.25,1), 2: (0.22,1), 3: (0.24,0.5), 4: (0.29,0.5)},\n",
    "        3: {1: (0.7,1), 2: (0.3,1), 3:(0,-1), 4:(0,-2)},\n",
    "        4: {1: (0.3,1), 2: (0.5,1), 3: (0.2,0.5), 4:(0,0)}\n",
    "    }\n",
    "r_to_R(MP_transistion_Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Write code to generate the stationary distribution for an MP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {0: 0.20367321054922707, 1: 0.796326789450773}, 1: {0: 0.45531133025903175, 1: 0.5446886697409682}}\n"
     ]
    }
   ],
   "source": [
    "def stationary_distribution(nS):\n",
    "    dic={}\n",
    "    for i in range(nS):\n",
    "        dic[i]={j:random.random() for j in range(nS)}\n",
    "    for i in range(nS):\n",
    "        a=0\n",
    "        for j in range(nS):\n",
    "            a=a+dic[i][j]\n",
    "        for j in range(nS):\n",
    "            dic[i][j]=dic[i][j]/a\n",
    "    return dic\n",
    "P=stationary_distribution(2)\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
