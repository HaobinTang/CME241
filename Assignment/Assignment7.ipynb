{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Assignment 7 by Haobin Tang\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Prove the Epsilon-Greedy Policy Improvement Theorem (we sketched the proof in Class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prove Monotonic ε-greedy Policy Improvement. Let $\\pi_i$ be an $\\epsilon$-greedy policy. Then, the $\\epsilon$-greedy policy with respect to $Q^{\\pi_i}$, denoted $\\pi_{i+1}$, is a monotonic improvement on policy $\\pi$. In other words,$V^{\\pi_{i+1}}$ ≥$V^{\\pi_i}$.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "V^{\\pi_{i+1}} (s)&=Q^{\\pi_i}(s,\\pi_{i+1}(s))\\\\\n",
    "&=\\sum_{a\\in A}\\pi_{i+1}(a|s)Q^{\\pi_i}(s,a)\\\\\n",
    "&=\\frac{\\epsilon}{|A|}\\sum_{a\\in A}Q^{\\pi_i}(s,a)+(1-\\epsilon){max}_{a'}Q^{\\pi_i}(s,a')\\\\\n",
    "&=\\frac{\\epsilon}{|A|}\\sum_{a\\in A}Q^{\\pi_i}(s,a)+(1-\\epsilon){max}_{a'}Q^{\\pi_i}(s,a')\\frac{1-\\epsilon}{1-\\epsilon}\\\\\n",
    "&=\\frac{\\epsilon}{|A|}\\sum_{a\\in A}Q^{\\pi_i}(s,a)+(1-\\epsilon){max}_{a'}Q^{\\pi_i}(s,a')\\sum_{a\\in A}\\frac{\\pi_{i}(a|s)-\\frac{\\epsilon}{|A|}}{1-\\epsilon}\\\\\n",
    "&=\\frac{\\epsilon}{|A|}\\sum_{a\\in A}Q^{\\pi_i}(s,a)+(1-\\epsilon)\\sum_{a\\in A}\\frac{\\pi_{i}(a|s)-\\frac{\\epsilon}{|A|}}{1-\\epsilon}{max}_{a'}Q^{\\pi_i}(s,a')\\\\\n",
    "&\\ge \\frac{\\epsilon}{|A|}\\sum_{a\\in A}Q^{\\pi_i}(s,a)+(1-\\epsilon)\\sum_{a\\in A}\\frac{\\pi_{i}(a|s)-\\frac{\\epsilon}{|A|}}{1-\\epsilon}Q^{\\pi_i}(s,a)\\\\\n",
    "&=\\sum_{a\\in A}\\pi_{i}(a|s)Q^{\\pi_i}(s,a)\\\\\n",
    "&=V^{\\pi_i}(s)\\\\\n",
    "\\end{aligned}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Provide (with clear mathematical notation) the defintion of GLIE (Greedy in the Limit with Infinite Exploration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.A policy $\\pi$ is greedy in the limit of in nite exploration (GLIE) if it satis es the following two properties:\n",
    "$$lim_{i\\to \\infty}N_i(s,a)\\to \\infty$$\n",
    "where $N_i(s,a)$ is the number of times action $a$ is taken at state $s$ up to and including episode $i$.\n",
    "$$$$\n",
    "2.The behavior policy converges to the policy that is greedy with respect to the learned Q-function. i.e. for all $s\\in S,a\\in A$,\n",
    "$$lim_{i\\to \\infty}\\pi_i(s,a)={argmax}_aq(s,a)$$with probability 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Implement the tabular Q-Learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I implement Q-learning using the project example.\n",
    "\n",
    "class Q_learninig():\n",
    "    def __init__(self):\n",
    "        self.iteration=2000000\n",
    "        \n",
    "        self.t_size=6\n",
    "        self.Price_size=100\n",
    "        self.N_size=50\n",
    "        self.start_price=80\n",
    "        self.gama=1\n",
    "        self.stepsize=0.1\n",
    "        self.epsilon=0.5\n",
    "\n",
    "    \n",
    "    def env(self):\n",
    "        Q={}\n",
    "        for i in range(self.t_size):\n",
    "            Q[i]={}\n",
    "            for j in range(self.Price_size):\n",
    "                Q[i][j]={}\n",
    "                for k in range(self.N_size):\n",
    "                    Q[i][j][k]={}\n",
    "                    for l in range(self.N_size):\n",
    "                        Q[i][j][k][l]=0\n",
    "                        \n",
    "        action={}\n",
    "        for i in range(self.t_size):\n",
    "            action[i]={}\n",
    "            for j in range(self.Price_size):\n",
    "                action[i][j]={}\n",
    "                for k in range(self.N_size):\n",
    "                    action[i][j][k]=random.randint(0,k)\n",
    "                    \n",
    "       \n",
    "        return Q, action\n",
    "    \n",
    "    \n",
    "    def simulator(self,Rt,Pt,Nt):\n",
    "        alpha=1\n",
    "        #error=np.random.randn()\n",
    "        #error=int(random.uniform(-2, 2))\n",
    "        #error=0\n",
    "        error=np.random.binomial(10, 0.5, size=None)-5\n",
    "        Pt=int(Pt-alpha*Nt-error)\n",
    "        if Pt<0:\n",
    "            Pt=0\n",
    "        Rt=Rt-Nt\n",
    "        reward=Nt*(Pt)\n",
    "\n",
    "        return Rt,Pt, reward\n",
    "           \n",
    "        \n",
    "        \n",
    "    def Q_learning_or_Sarsa(self, method=\"q_learning\"):\n",
    "        #initialize\n",
    "        Q, action = self.env()\n",
    "        \n",
    "        iteration=[]\n",
    "        Q_plot=[]\n",
    "        for i in range(self.iteration):\n",
    "             # Using Sigmoid function as epsilon decrease\n",
    "            epsilon=1-(1/(1+math.exp(5-10*i/self.iteration)))\n",
    "            \n",
    "            #Set s0 as the starting state\n",
    "            t=0\n",
    "            Pt=self.start_price\n",
    "            Rt=self.N_size-1\n",
    "            \n",
    "            best_action=max(Q[t][Pt][Rt], key=Q[t][Pt][Rt].get)\n",
    "            if random.uniform(0, 1)<epsilon:\n",
    "                action[t][Pt][Rt]= random.randint(0,Rt)\n",
    "            else:\n",
    "                action[t][Pt][Rt]= best_action \n",
    "            \n",
    "            while t<self.t_size-1 and Rt>0:  # loop until episode terminates\n",
    "                Nt=action[t][Pt][Rt] # Sample action at from policy π(st)\n",
    "                \n",
    "                #print (t,Pt,Rt,Nt)\n",
    "                #Take action at and observe reward rt and next state st+1\n",
    "                Rt_next,Pt_next, reward= self.simulator(Rt,Pt,Nt)\n",
    "                t_next=t+1\n",
    "                # update Q\n",
    "                    # find next state Qmax\n",
    "                \n",
    "                best_action=max(Q[t_next][Pt_next][Rt_next], key=Q[t_next][Pt_next][Rt_next].get)\n",
    "                \n",
    "                #print (t_next,Pt_next,Rt_next,Nt,best_action)\n",
    "                if method==\"q_learning\":\n",
    "                    Qmax=Q[t_next][Pt_next][Rt_next][best_action]\n",
    "                    Qnow= Q[t][Pt][Rt][Nt]\n",
    "                    Q[t][Pt][Rt][Nt]=Qnow+self.stepsize*(reward+self.gama*Qmax-Qnow)\n",
    "                \n",
    "                if method==\"sarsa\":\n",
    "                    Nt_next=action[t_next][Pt_next][Rt_next]\n",
    "                    Qnext= Q[t_next][Pt_next][Rt_next][Nt_next]\n",
    "                    Q[t][Pt][Rt][Nt]=Qnow+self.stepsize*(reward+self.gama*Qnext-Qnow)\n",
    "\n",
    "                #update S\n",
    "                t=t_next\n",
    "                Pt=Pt_next\n",
    "                Rt=Rt_next\n",
    "                \n",
    "                if random.uniform(0, 1)<epsilon:\n",
    "                    action[t][Pt][Rt]= random.randint(0,Rt)\n",
    "                else:\n",
    "                    action[t][Pt][Rt]= best_action \n",
    "            \n",
    "            #for plotting\n",
    "            if i%(self.iteration/10)==0:\n",
    "                first_day_sell=max(Q[0][self.start_price][self.N_size-1], key=Q[0][self.start_price][self.N_size-1].get)\n",
    "                print (i//(self.iteration/10),\"/10\", \"first_day_sell=\",first_day_sell, \"Q_value=\",Q[0][self.start_price][self.N_size-1][first_day_sell])\n",
    "    \n",
    "                shares_to_sell_everyday=[first_day_sell] \n",
    "                sell=first_day_sell\n",
    "                remain=self.N_size-1-sell\n",
    "                price=self.start_price-sell\n",
    "                for j in range(1,self.t_size):\n",
    "                    Q_value=0\n",
    "                    sell=max(Q[j][price][remain], key=Q[j][price][remain].get)\n",
    "                    shares_to_sell_everyday.append(sell)\n",
    "                    remain=remain-sell\n",
    "                    price=price-sell\n",
    "                    #print(remain)\n",
    "                    '''\n",
    "                    for k in range(self.Price_size):\n",
    "                        temp=max(Q[j][k][remain], key=Q[j][k][remain].get)\n",
    "                        if Q[j][k][remain][temp]>Q_value:\n",
    "                            sell=temp\n",
    "                            Q_value=Q[j][k][remain][temp]\n",
    "                    if sell>remain:\n",
    "                        sell=0\n",
    "                    shares_to_sell_everyday.append(sell)\n",
    "                    #print(sell)\n",
    "                    remain=remain-sell\n",
    "                '''\n",
    "                print(shares_to_sell_everyday) \n",
    "                \n",
    "            if i%(self.iteration/1000)==0:\n",
    "                iteration.append(i)\n",
    "                first_day_sell=max(Q[0][self.start_price][self.N_size-1], key=Q[0][self.start_price][self.N_size-1].get)\n",
    "                Q_plot.append(Q[0][self.start_price][self.N_size-1][first_day_sell])\n",
    "    \n",
    "        \n",
    "        plt.plot(iteration,Q_plot)\n",
    "        #print(iteration)\n",
    "        return Q, action"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
