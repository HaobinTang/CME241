{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Assignment 2 by Haobin Tang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Write the Bellman equation for MRP Value Function and code to calculate MRP Value Function (based on Matrix inversion method you learnt in this lecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$V_t(s) = E[G_t|s_t = s]$$\n",
    "$$G_t = \\sum_{i=t}^{H-1}γ^{i−t}r_i , ∀0≤t≤H−1$$\n",
    "Using this, the fact that the horizon is inifnite, and using the stationary Markov property we have for any state $s ∈ S$:\n",
    "$$V(s) = V_0(s)=E[G_0|s_0 =s]=E[\\sum_{i=0}γ^ir_i|s_0=s] =E[r0|s0 =s]+\\sum_{i=0}γ^iE[r_i|s_0 =s]$$\n",
    "\n",
    "There is a nice interpretation of the final result of the above equation, namely that the  rssult term $R(s)$ is the immediate reward while the second term $γ \\sum_{s′∈S} P(s′|s)V (s′) $is the discounted sum of future rewards. The value function $V (s)$ is the sum of these two quantities. As $|S| < ∞$, it is possible to write this equation in matrix form as:\n",
    "\n",
    "$$V =R+γPV $$\n",
    "\n",
    "We can solve by Matrix inversion:\n",
    "\n",
    "$$V = (I − γP)^{−1}R$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use the data in Assignment 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MP_transistion_Data={\n",
    "        1: {1: 0.1, 2: 0.6, 3: 0.1, 4: 0.2},\n",
    "        2: {1: 0.25, 2: 0.22, 3: 0.24, 4: 0.29},\n",
    "        3: {1: 0.7, 2: 0.3, 3:0, 4:0},\n",
    "        4: {1: 0.3, 2: 0.5, 3: 0.2, 4:0}\n",
    "    }\n",
    "\n",
    "Reward={1:1,2:1,3:1,4:-2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Solve_V(MP_transistion_Data,Reward,gama):\n",
    "    n=len(MP_transistion_Data)\n",
    "    V_matrix=np.zeros((n,n))\n",
    "    R_vector=np.zeros(n)\n",
    "    all_one=np.ones((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            V_matrix[i][j]=MP_transistion_Data[i+1][j+1]*gama\n",
    "    for i in range(n):\n",
    "        R_vector[i]=Reward[i+1]\n",
    "    \n",
    "    Result=np.dot(R_vector,np.linalg.inv(all_one-np.transpose(V_matrix)))\n",
    "    \n",
    "    return Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the result of value function is \n",
      "[ -1.30222105   1.29340737  12.7688172  -12.2311828 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"the result of value function is \" )\n",
    "print(Solve_V(MP_transistion_Data,Reward,0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Write out the MDP definition, Policy definition and MDP Value Function definition (in LaTeX) in your own style/notation (so you really internalize these concepts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now in a position to de ne a Markov decision process (MDP). A MDP inherits the basic struc- ture of a Markov reward process with some important key di erences, together with the speci cation of a set of actions that an agent can take from each state. It is formally represented using the tuple $(S, A, P, R, γ)$ which are listed below:\n",
    "\n",
    "$• S : $A  finite state space.\n",
    "\n",
    "$• A : $A  finite set of actions which are available from each state s.\n",
    "\n",
    "$• P : $A transition probability model that specifies$ P (s′|s, a)$.\n",
    "\n",
    "$• R : $A reward function that maps a state-action pair to rewards (real numbers), $i.e. R : S×A → R$. \n",
    "\n",
    "$• γ: $Discount factor $γ ∈ [0, 1]$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that the policy may be varying with time, which is especially true in the case of finite horizon MDPs. We will denote a generic policy by the boldface symbol $π$, dfined as the infinite dimensional tuple $π = (π0, π1, . . . )$, where $π_t$ refers to the policy at time $t$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state value function $V_t^{π(s)}$ for a state $s ∈ S$ is defined as the expected return starting from the state $s_t = s$ at time t and following policy $π$, and is given by the expression $V_t^{π(s)} = E^π[G_t|s_t = s]$, where $E_π$ denotes that the expectation is taken with respect to the policy $π$. Frequently we will drop the subscript $π$ in the expectation to simplify notation going forward. Thus $E$ will mean expectation with respect to the policy unless specified otherwise, and so we can write\n",
    "$$V_t^{π(s)} = E[G_t|s_t = s] $$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Think about the data structure/class design (in Python 3) to represent MDP, Policy, Value Function, and implement them with clear type definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy is a dictionary, which corresponds to each states a distribution of the action of that states, which should be float. The type of value function should be float."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Write code to convert/cast the r(s,s',a) definition of MDP to the R(s,a) definition of MDP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First to explain the data structure:\n",
    "    For each pair of states in [1, nS] and actions in [1, nA], P[state][action] is a tuple of the form (probability, nextstate, reward, terminal) where\n",
    "\t\t\t- probability: float\n",
    "\t\t\t\tthe probability of transitioning from \"state\" to \"nextstate\" with \"action\"\n",
    "\t\t\t- nextstate: int\n",
    "\t\t\t\tdenotes the state we transition to (in range [0, nS - 1])\n",
    "\t\t\t- reward: int\n",
    "\t\t\t\teither 0 or 1, the reward for transitioning from \"state\" to\n",
    "\t\t\t\t\"nextstate\" with \"action\"\n",
    "\t\t\t- terminal: bool\n",
    "\t\t\t  True when \"nextstate\" is a terminal state, False otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_to_R_2(r):\n",
    "    dic1={}\n",
    "    for i in r:\n",
    "        dic1[i]={}\n",
    "        for j in r[i]:\n",
    "            dic1[i][j]=0\n",
    "            for a,b,c in r[i][j]:\n",
    "                dic1[i][j] +=a*c\n",
    "    return dic1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {1: 1.0999999999999999, 2: 1.1, 3: 1.9000000000000001},\n",
       " 2: {1: 2.3000000000000003, 2: 2.9, 3: 0.10000000000000003},\n",
       " 3: {1: -0.09999999999999981, 2: 1.1, 3: 0.7},\n",
       " 4: {1: 1.3, 2: -0.6999999999999998, 3: 0.5}}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data={\n",
    "    1:{1:[(0.2,1,5),(0.4,2,1),(0.3,3,-1),(0.1,4,0)],\n",
    "       2:[(0.3,1,3),(0.3,2,2),(0.4,3,-1),(0,4,0)],\n",
    "       3:[(0.4,1,4),(0.2,2,3),(0.3,3,-1),(0.1,4,0)]},\n",
    "    2:{1:[(0.2,1,5),(0.4,2,4),(0.3,3,-1),(0.1,4,0)],\n",
    "       2:[(0.3,1,6),(0.3,2,5),(0.4,3,-1),(0,4,0)],\n",
    "       3:[(0.4,1,-1),(0.2,2,4),(0.3,3,-1),(0.1,4,0)]},\n",
    "    3:{1:[(0.2,1,-5),(0.4,2,3),(0.3,3,-1),(0.1,4,0)],\n",
    "       2:[(0.3,1,3),(0.3,2,2),(0.4,3,-1),(0,4,0)],\n",
    "       3:[(0.4,1,2),(0.2,2,1),(0.3,3,-1),(0.1,4,0)]},\n",
    "    4:{1:[(0.2,1,8),(0.4,2,0),(0.3,3,-1),(0.1,4,0)],\n",
    "       2:[(0.3,1,-6),(0.3,2,5),(0.4,3,-1),(0,4,0)],\n",
    "       3:[(0.4,1,-2),(0.2,2,8),(0.3,3,-1),(0.1,4,0)]}\n",
    "}\n",
    "\n",
    "r_to_R_2(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.Write code to create a MRP given a MDP and a Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MDR_to_MRP(r, policy):\n",
    "    dic1={}\n",
    "    for i in r:\n",
    "        dic1[i]=[]\n",
    "        reward=0\n",
    "        pro=0\n",
    "        for j in r:\n",
    "            for action in r[i]:\n",
    "                reward+=r[i][action][j-1][2]*policy[i][action]\n",
    "                pro+=r[i][action][j-1][0]*policy[i][action]\n",
    "                \n",
    "            dic1[i].append((pro,j,reward))\n",
    "            \n",
    "    return dic1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: [(0.3, 1, 3), (0.6, 2, 5), (1.0, 3, 4), (1.0, 4, 4)],\n",
       " 2: [(0.30000000000000004, 1, 4.3999999999999995),\n",
       "  (0.6000000000000001, 2, 9.0),\n",
       "  (0.9600000000000002, 3, 8.000000000000002),\n",
       "  (1.0000000000000002, 4, 8.000000000000002)],\n",
       " 3: [(0.27, 1, -1.2000000000000002),\n",
       "  (0.6000000000000001, 2, 1.0999999999999999),\n",
       "  (0.9300000000000002, 3, 0.09999999999999987),\n",
       "  (1.0000000000000002, 4, 0.09999999999999987)],\n",
       " 4: [(0.3, 1, -6), (0.6, 2, -1), (1.0, 3, -2), (1.0, 4, -2)]}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy={\n",
    "    1:{1:0,2:1,3:0},\n",
    "    2:{1:0.2,2:0.6,3:0.2},\n",
    "    3:{1:0.5,2:0.3,3:0.2},\n",
    "    4:{1:0,2:1,3:0}\n",
    "}\n",
    "MDR_to_MRP(data, policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Write out all 8 MDP Bellman Equations and also the transformation from Optimal Action-Value function to Optimal Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_π(s) = \\sum_{a∈A}π(a|s)q_π(s,a)$$\n",
    "$$q_π(s,a)=R_s^a+γ\\sum_{s'∈S}P^a_{ss'}v_π(s')$$\n",
    "$$v_π(s) =\\sum_{a∈A}π(a|s)(R_s^a+γ\\sum_{s'∈S}P^a_{ss'}v_π(s'))$$\n",
    "$$q_π(s,a)=R_s^a+γ\\sum_{s'∈S}P^a_{ss'}(\\sum_{a∈A}π(a|s')q_π(s',a'))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_*(s) = max_{a∈A}q_*(s,a)$$\n",
    "$$q_*(s,a)=R_s^a+γ\\sum_{s'∈S}P^a_{ss'}v_*(s')$$\n",
    "$$v_*(s) = max_{a∈A}R_s^a+γ\\sum_{s'∈S}P^a_{ss'}v_*(s')$$\n",
    "$$q_*(s,a)=R_s^a+γ\\sum_{s'∈S}P^a_{ss'}max_{a∈A}q_*(s',a')$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
