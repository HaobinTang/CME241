{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Assignment 3 by Haobin Tang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explaination:\n",
    "    For policy_evaluation, policy_improvement, policy_iteration and value_iteration,\n",
    "the parameters P, nS, nA, gamma are defined as follows:\n",
    "\n",
    "\tP: nested dictionary\n",
    "\t\tFrom gym.core.Environment\n",
    "\t\tFor each pair of states in [1, nS] and actions in [1, nA], P[state][action] is a\n",
    "\t\ttuple of the form (probability, nextstate, reward, terminal) where\n",
    "\t\t\t- probability: float\n",
    "\t\t\t\tthe probability of transitioning from \"state\" to \"nextstate\" with \"action\"\n",
    "\t\t\t- nextstate: int\n",
    "\t\t\t\tdenotes the state we transition to (in range [0, nS - 1])\n",
    "\t\t\t- reward: int\n",
    "\t\t\t\teither 0 or 1, the reward for transitioning from \"state\" to\n",
    "\t\t\t\t\"nextstate\" with \"action\"\n",
    "\t\t\t- terminal: bool\n",
    "\t\t\t  True when \"nextstate\" is a terminal state, False otherwise\n",
    "\tnS: int\n",
    "\t\tnumber of states in the environment\n",
    "\tnA: int\n",
    "\t\tnumber of actions in the environment\n",
    "\tgamma: float\n",
    "\t\tDiscount factor. Number in range [0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# policy_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(P, nS, nA, policy, gamma=0.9, tol=1e-3):\n",
    "    value_function = np.zeros(nS)\n",
    "    while True:\n",
    "        prev_value_function = np.zeros(nS)\n",
    "        for i in range(nS):\n",
    "            #policy_s=policy[i]\n",
    "            for prob, next_state, reward, terminal in P[i][policy[i]]:\n",
    "                prev_value_function[i] += prob * (reward+ gamma * value_function[next_state])\n",
    "        var = np.linalg.norm(prev_value_function-value_function, ord=np.inf)\n",
    "        if var > tol:\n",
    "            value_function = prev_value_function\n",
    "        else :\n",
    "            break    \n",
    "    return value_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# policy_improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(P, nS, nA, value_from_policy, policy, gamma=0.9):\n",
    "    new_policy = np.zeros(nS, dtype='int')\n",
    "    for i in range(nS):\n",
    "        Qvalue = np.zeros(nA)\n",
    "        for j in range(nA):\n",
    "            for prob, next_state, reward, terminal in P[i][j]:\n",
    "                #if terminal:\n",
    "                 #   Qvalue[j]+= prob * reward\n",
    "                #else:\n",
    "                Qvalue[j]+= prob * (reward + gamma * value_from_policy[next_state])\n",
    "        new_policy[i]=np.argmax(Qvalue)   \n",
    "    return new_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# policy_iteration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(P, nS, nA, gamma=0.9, tol=10e-3):\n",
    "    value_function = np.zeros(nS)\n",
    "    policy = np.zeros(nS, dtype=int)\n",
    "    while True:\n",
    "        new_policy=np.zeros(nS, dtype=int)\n",
    "        value_function=policy_evaluation(P, nS, nA, policy, gamma=0.9, tol=10e-3)\n",
    "        new_policy=policy_improvement(P, nS, nA, value_function, policy, gamma=0.9)\n",
    "\n",
    "        if (policy==new_policy).all():\n",
    "           # np.array_equal()\n",
    "            break\n",
    "        else:\n",
    "            policy=new_policy       \n",
    "    return value_function, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# value_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(P, nS, nA, gamma=0.9, tol=1e-3):  \n",
    "    value_function = np.zeros(nS)\n",
    "    policy = np.zeros(nS, dtype=int)\n",
    "    while True:\n",
    "        prev_value_function = np.ones(nS)\n",
    "        for i in range(nS):\n",
    "            Qvalue = np.zeros(nA)\n",
    "            \n",
    "            for j in range(nA):\n",
    "                for prob, next_state, reward, terminal in P[i][j]:\n",
    "                    Qvalue[j] += prob * (reward + gamma * value_function[next_state])\n",
    "            \n",
    "            prev_value_function[i]=np.max(Qvalue)\n",
    "            policy[i]=np.argmax(Qvalue)\n",
    "            \n",
    "        var=np.linalg.norm(prev_value_function-value_function, ord=np.inf)\n",
    "        if var>tol:\n",
    "            value_function=prev_value_function\n",
    "        else:\n",
    "            break   \n",
    "    return value_function, policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
