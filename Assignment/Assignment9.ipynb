{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Assignment 9 by Haobin Tang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Write Proof (with precise notation) of the Policy Gradient Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, in order to learn a policy, we have focused on value-based approaches where we  nd the\n",
    "optimal state value function or state-action value function with parameters $\\theta$,\n",
    "$$V_\\theta(s)=V^\\pi(s)$$\n",
    "In this setting, our goal is to directly  nd the policy with the highest value function $V^\\pi$, rather than  rst  nding the value-function of the optimal policy and then extracting the policy from it. Instead of the policy being a look-up table from states to actions, we will consider stochastic policies that are parameterized. Finding a good policy requires two parts:\n",
    "\n",
    "1. Good policy parameterization: our function approximation and state/action representations must be expressive enough\n",
    "2. Erffective search: we must be able to  nd good parameters for our policy function approximation\n",
    "\n",
    "Policy-based RL has a few advantages over value-based RL:\n",
    "1. Better convergence properties \n",
    "2. Effectiveness in high-dimensional or continuous action spaces, e.g. robotics.\n",
    "3. Ability to learn stochastic policies. \n",
    "\n",
    "\n",
    "The disadvantages of policy-based RL methods are:\n",
    "1. They typically converge to locally rather than globally optimal policies, since they rely on gradient descent.\n",
    "2. Evaluating a policy is typically data ine cient and high variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the sum notation for simplicity to prove instead of integral.\n",
    "\n",
    "In an episodic environment, a natural measurement is the start value of the policy, which is the expected value of the start state:\n",
    "$$J_1(\\theta)=V^{\\pi_\\theta}(s)=\\mathbb{E}_{\\pi_\\theta}[v_1]$$\n",
    "Let us define $V(\\theta)$ to be the objective function we wish to maximize over $\\theta$. Policy gradient methods search for a local maximum in $V(\\theta)$ by ascending the gradient of the policy, w.r.t parameters $\\theta$\n",
    "$$\\Delta\\theta=\\alpha\\nabla_\\theta V(\\theta) $$\n",
    "Let us set the objective function $V(\\theta)$ to be the expected rewards for an episode,\n",
    "$$V(\\theta)=\\mathbb{E}_{(s_t,a_t)\\sim\\pi_\\theta}[\\sum_{t=0}^{T}R(s_t,a_t)]=\\mathbb{E}_{\\tau \\sim\\pi_\\theta}[R(\\tau)]=\\sum_\\tau P(\\tau;\\theta)R(\\tau)$$\n",
    "where $\\tau$ is a trajectory,\n",
    "$$\\tau=((s_0, a_0, r_0, · · · , s_{T −1}, a_{T −1}, r_{T −1}, s_T ))$$\n",
    "If we can mathematically compute the policy gradient $\\nabla_\\theta \\pi_\\theta(a|s)$, then we can go right ahead and compute the gradient of this objective function with respect to $\\theta$:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\nabla_\\theta V(\\theta)&=\\nabla_\\theta\\sum_\\tau P(\\tau;\\theta)R(\\tau)\\\\\n",
    "&=\\sum_\\tau\\nabla_\\theta P(\\tau;\\theta)R(\\tau)\\\\\n",
    "&=\\sum_\\tau\\frac{P(\\tau;\\theta)}{P(\\tau;\\theta)}\\nabla_\\theta P(\\tau;\\theta)R(\\tau)\\\\\n",
    "&=\\sum_\\tau P(\\tau;\\theta)\\frac{\\nabla_\\theta P(\\tau;\\theta)}{P(\\tau;\\theta)}R(\\tau)\\\\\n",
    "&=\\sum_\\tau P(\\tau;\\theta)R(\\tau)\\nabla_\\theta log(P(\\tau;\\theta))\\\\\n",
    "&=\\mathbb{E}_{\\tau \\sim\\pi_\\theta}[R(\\tau)\\nabla_\\theta log(P(\\tau;\\theta))]\\\\\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, computing $\\nabla_\\theta log(P(\\tau;\\theta)) $\n",
    "$$\\begin{aligned}\n",
    "\\nabla_\\theta log(P(\\tau;\\theta))&=\\nabla_\\theta log[\\mu(s_0) \\prod_{t=0}^{T-1}\\pi_\\theta(a_t|s_t)P(s_{t+1}|a_t|s_t)] \\\\\n",
    "&=\\nabla_\\theta [log\\mu(s_0) \\sum_{t=0}^{T-1}log\\pi_\\theta(a_t|s_t)+logP(s_{t+1}|a_t|s_t)] \\\\\n",
    "&=\\sum_{t=0}^{T-1}\\nabla_\\theta log\\pi_\\theta(a_t|s_t) \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "Working with $log(P(\\tau;\\theta)) $ instead of $(P(\\tau;\\theta)) $ allows us to represent the gradient without reference to the initial state distribution, or even the environment dynamics model!\n",
    "\n",
    "For now we get $$\\nabla_\\theta V(\\theta)=\\mathbb{E}_{\\pi_\\theta} [R(\\tau)\\nabla_\\theta log\\pi_\\theta(a|s)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theorem  For any di erentiable policy $\\pi_\\theta(a|s)$ and for any of the policy objective functions, the policy gradient is\n",
    "$$\\nabla_\\theta V(\\theta)=\\mathbb{E}_{\\pi_\\theta} [Q^{\\pi_\\theta}(a|s)\\nabla_\\theta log\\pi_\\theta(a|s)]$$\n",
    "\n",
    "Notice that the rewards $R(\\tau^{(i)})$ are treated as a single number which is a function of an entire trajectory $\\tau^{(i)}$. We can break this down into the sum of all the rewards encountered in the trajectory,\n",
    "$$R(\\tau)=\\sum_{t=0}^{T-1}R(s_t,a_t)$$\n",
    "\n",
    "Using this knowledge, we can derive the gradient estimate for a single reward term $r_t′$ in exactly the same way we derived equation :\n",
    "$$\\nabla_\\theta \\mathbb{E}_{\\pi_\\theta}[r_t′]=\\mathbb{E}_{\\pi_\\theta}[r_t′\\sum_{t=0}^{t'}\\nabla_\\theta log\\pi_\\theta(a_t|s_t)]$$\n",
    "\n",
    "Since $\\sum_{t’=t}^{T-1}r_{t'}$ is the return $G_t$, we can sum this up over all time steps for a trajectory to get\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\nabla_\\theta V(\\theta)&=\\mathbb{E}_{\\tau \\sim\\pi_\\theta}[R(\\tau)]\\\\\n",
    "&=\\mathbb{E}_{\\pi_\\theta}[\\sum_{t’=0}^{T-1}r_{t'} \\sum_{t=0}^{t'}\\nabla_\\theta log(\\pi_\\theta(a_t|s_t)]\\\\\n",
    "&=\\mathbb{E}_{\\pi_\\theta}[\\sum_{t=0}^{T-1}\\nabla_\\theta log(\\pi_\\theta(a_t|s_t)\\sum_{t'=t}^{T-1}r_{t'}]\\\\\n",
    "&=\\mathbb{E}_{\\pi_\\theta}[\\sum_{t=0}^{T-1}\\nabla_\\theta log(\\pi_\\theta(a_t|s_t)G_t] \n",
    "\\end{aligned}$$\n",
    "\n",
    "The main idea is that the policy's choice at a particular time step $t$ only a ects rewards received in later steps of the episode, and has no effect on rewards received in previous time steps. Our original expression in the above equation did not take this into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Derive the score function for softmax policy (for finite set of actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expression $log(\\pi_\\theta(a|s))$ is known as the score function.\n",
    "The $\\nabla_\\theta\\pi_\\theta(a|s)$ for finite action spaces is \n",
    "$$\\nabla_\\theta\\pi_\\theta(a|s)=\\frac{e^{\\theta^T*\\phi(s,a)}}{\\sum_b e^{\\theta^T*\\phi(s,b)}}$$\n",
    "$$\\nabla_\\theta log(\\pi_\\theta(a|s))=\\nabla_\\theta(\\theta^T*\\phi(s,a)-{\\sum_b \\theta^T*\\phi(s,b)})\\\\\n",
    "=\\phi(s,a)-\\frac{\\sum_b\\phi(s,b)*e^{\\theta^T*\\phi(s,b)}}{\\sum_b e^{\\theta^T*\\phi(s,b)}}\\\\\n",
    "=\\phi(s,a)-\\sum_b \\pi_\\theta(b|s)*\\phi(s,b)\n",
    "$$\n",
    "\n",
    "We also can derive \n",
    "$$\\mathbb{E}_{b}[log(\\pi_\\theta(a|s))]=0$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Write code for the REINFORCE Algoithm (Monte-Carlo Policy Gradient Algorithm, i.e., no Critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a REINFORCE Algorithm framework I wrote as homework in CS234 using TensorFlow.\n",
    "\n",
    "class REINFORCE(object):\n",
    "    def __init__(self, env, logger=None):\n",
    "        self.batch_size=1000\n",
    "        self.episode_max_length=10\n",
    "        self.num_seq_per_batch=50\n",
    "        self.gamma=0.9\n",
    "    def add_placeholders_op(self):\n",
    "        \"\"\"\n",
    "        Add placeholders for observation, action, and advantage:\n",
    "            self.observation_placeholder, type: tf.float32\n",
    "            self.action_placeholder, type: depends on the self.discrete\n",
    "            self.advantage_placeholder, type: tf.float32\n",
    "        \"\"\"\n",
    "        self.observation_placeholder = tf.placeholder(tf.float32, shape = [None, self.observation_dim])\n",
    "        self.action_placeholder = tf.placeholder(tf.int64, shape = [None,])\n",
    "        self.advantage_placeholder = tf.placeholder(tf.float32, shape = [None,])\n",
    "    def build_policy_network_op(self, scope = \"policy_network\"):\n",
    "        \"\"\"\n",
    "        Build the policy network, construct the tensorflow operation to sample\n",
    "        actions from the policy network outputs, and compute the log probabilities\n",
    "        of the actions taken (for computing the loss later). These operations are\n",
    "        stored in self.sampled_action and self.logprob. Must handle both settings\n",
    "        of self.discrete.\n",
    "\n",
    "        Args:\n",
    "              scope: the scope of the neural network\n",
    "        \"\"\"\n",
    "        action_logits = build_mlp(self.observation_placeholder, self.action_dim, scope, self.pa.n_layers, self.pa.layer_size)\n",
    "        self.sampled_action = tf.squeeze(tf.multinomial(action_logits, 1), axis = 1)\n",
    "        self.logprob = - tf.nn.sparse_softmax_cross_entropy_with_logits(labels = self.action_placeholder, logits = action_logits)\n",
    "    def add_loss_op(self):\n",
    "        \"\"\"\n",
    "        Compute the loss, averaged for a given batch.\n",
    "        The update for REINFORCE with advantage:\n",
    "        θ = θ + α ∇_θ log π_θ(a_t|s_t) A_t\n",
    "        \"\"\"\n",
    "        self.loss =  -tf.reduce_mean(self.logprob * self.advantage_placeholder)\n",
    "\n",
    "    def add_optimizer_op(self):\n",
    "        \"\"\"\n",
    "        Set 'self.train_op' using AdamOptimizer\n",
    "        \"\"\"\n",
    "        self.train_op = tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.loss)\n",
    "\n",
    "    def add_baseline_op(self, scope = \"baseline\"):\n",
    "        \"\"\"\n",
    "        Build the baseline network within the scope.\n",
    "        Use build_mlp with the same parameters as the policy network to\n",
    "        get the baseline estimate, and setup a target placeholder and\n",
    "        an update operation so the baseline can be trained.\n",
    "\n",
    "        Args:\n",
    "          scope: the scope of the baseline network\n",
    "        \"\"\"\n",
    "        self.baseline = tf.squeeze(build_mlp(self.observation_placeholder, 1, scope, self.pa.n_layers, self.pa.layer_size))\n",
    "        self.baseline_target_placeholder = tf.placeholder(tf.float32, shape = [None, ])\n",
    "        loss = tf.losses.mean_squared_error(labels = self.baseline_target_placeholder, predictions = self.baseline)\n",
    "        self.update_baseline_op = tf.train.AdamOptimizer(learning_rate = self.lr).minimize(loss)\n",
    "\n",
    "    def build(self):\n",
    "        \"\"\"\n",
    "        Build the model by adding all necessary variables.\n",
    "        \"\"\"\n",
    "        # add placeholders\n",
    "        self.add_placeholders_op()\n",
    "        # create policy net\n",
    "        self.build_policy_network_op()\n",
    "        # add square loss\n",
    "        self.add_loss_op()\n",
    "        # add optmizer for the main networks\n",
    "        self.add_optimizer_op()\n",
    "\n",
    "        # add baseline\n",
    "        if self.pa.use_baseline:\n",
    "            self.add_baseline_op()\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"\n",
    "        Assumes the graph has been constructed (have called self.build())\n",
    "        Creates a tf Session and run initializer of variables\n",
    "        \"\"\"\n",
    "        # create tf session\n",
    "        self.sess = tf.Session()\n",
    "        # initiliaze all variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess.run(init)\n",
    "\n",
    "\n",
    "    def sample_path(self, env, num_episodes = None):\n",
    "        \"\"\"\n",
    "        Sample trajectories from the environment.\n",
    "\n",
    "        Args:\n",
    "            env: a built simulator environment\n",
    "            num_episodes: the number of episodes to be sampled\n",
    "              if none, sample one batch (size indicated by config file)\n",
    "        Returns:\n",
    "          paths: a list of paths. Each path in paths is a dictionary with\n",
    "              path[\"observation\"] a numpy array of ordered observations in the path\n",
    "              path[\"actions\"] a numpy array of the corresponding actions in the path\n",
    "              path[\"reward\"] a numpy array of the corresponding rewards in the path\n",
    "          total_rewards: the sum of all rewards encountered during this \"path\"\n",
    "        \"\"\"\n",
    "        episode = 0\n",
    "        episode_rewards = []\n",
    "        paths = []\n",
    "        t = 0\n",
    "\n",
    "        while (num_episodes or t < self.batch_size):\n",
    "            env.reset()\n",
    "            states, actions, rewards= [], [], []\n",
    "            state = env.observe()\n",
    "            episode_reward = 0\n",
    "\n",
    "            for step in range(self.episode_max_length):\n",
    "                states.append(state)\n",
    "                action = self.sess.run(self.sampled_action, feed_dict={self.observation_placeholder : states[-1][None]})[0]\n",
    "                actions.append(action)\n",
    "                state, reward, done = env.step(action)\n",
    "                rewards.append(reward)\n",
    "                episode_reward += reward\n",
    "                t += 1\n",
    "                if (done or step == self.episode_max_length-1):\n",
    "                    episode_rewards.append(episode_reward)\n",
    "                    break\n",
    "                if (not num_episodes) and t == self.batch_size:\n",
    "                    break\n",
    "\n",
    "            path = {\"observation\" : np.array(states),\n",
    "                            \"reward\" : np.array(rewards),\n",
    "                            \"action\" : np.array(actions),\n",
    "                            \"info\"   : info}\n",
    "            paths.append(path)\n",
    "            episode += 1\n",
    "            if num_episodes and episode >= self.num_seq_per_batch:\n",
    "                break\n",
    "\n",
    "        return paths, episode_rewards\n",
    "    def get_returns(self, paths):\n",
    "        \"\"\"\n",
    "        Calculate the returns G_t for each timestep\n",
    "        After acting in the environment, we record the observations, actions, and\n",
    "        rewards. To get the advantages that we need for the policy update, we have\n",
    "        to convert the rewards into returns, G_t, which are themselves an estimate\n",
    "        of Q^π (s_t, a_t):\n",
    "                    G_t = r_t + γ r_{t+1} + γ^2 r_{t+2} + ... + γ^{T-t} r_T\n",
    "            where T is the last timestep of the episode.\n",
    "        Args:\n",
    "              paths: recorded sample paths.  See sample_path() for details.\n",
    "        Return:\n",
    "              returns: return G_t for each timestep\n",
    "        \"\"\"\n",
    "        all_returns = []\n",
    "        for path in paths:\n",
    "            rewards = path[\"reward\"]\n",
    "            returns = []\n",
    "            T = len(rewards)\n",
    "            for i in range(T):\n",
    "                gammas = np.logspace(0, T - i, num = T - i, base = self.gamma, endpoint = False)\n",
    "                r_t = np.dot(rewards[i:], gammas)\n",
    "                returns.append(r_t)\n",
    "            all_returns.append(returns)\n",
    "        returns = np.concatenate(all_returns)\n",
    "        return returns\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Performs training\n",
    "        \"\"\"\n",
    "        last_eval = 0\n",
    "        last_record = 0\n",
    "        scores_eval = []\n",
    "\n",
    "        self.init_averages()\n",
    "        scores_eval = [] # list of scores computed at iteration time\n",
    "\n",
    "        for t in range(self.pa.num_batches):\n",
    "            \n",
    "            # collect a minibatch of samples\n",
    "            paths, total_rewards = self.sample_path(self.env)\n",
    "            scores_eval = scores_eval + total_rewards\n",
    "            observations = np.concatenate([path[\"observation\"] for path in paths])\n",
    "            actions = np.concatenate([path[\"action\"] for path in paths])\n",
    "            rewards = np.concatenate([path[\"reward\"] for path in paths])\n",
    "            # compute Q-val estimates (discounted future returns) for each time step\n",
    "            returns = self.get_returns(paths)\n",
    "            advantages = self.calculate_advantage(returns, observations)\n",
    "\n",
    "            # run training operations\n",
    "            if self.pa.use_baseline:\n",
    "                self.update_baseline(returns, observations)\n",
    "            self.sess.run(self.train_op, feed_dict={\n",
    "                          self.observation_placeholder : observations,\n",
    "                          self.action_placeholder : actions,\n",
    "                          self.advantage_placeholder : advantages})\n",
    "\n",
    "            # tf stuff\n",
    "            if (t % self.pa.summary_freq == 0):\n",
    "                self.update_averages(total_rewards, scores_eval)\n",
    "                self.record_summary(t)\n",
    "\n",
    "            # compute reward statistics for this batch and log\n",
    "            avg_reward = np.mean(total_rewards)\n",
    "            sigma_reward = np.sqrt(np.var(total_rewards) / len(total_rewards))\n",
    "            msg = \"Average reward: {:04.2f} +/- {:04.2f}\".format(avg_reward, sigma_reward)\n",
    "            self.logger.info(msg)\n",
    "\n",
    "        self.logger.info(\"- Training done.\")\n",
    "        export_plot(scores_eval, \"Score\", self.pa.env_name, self.pa.plot_output)\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Apply procedures of training for a PG.\n",
    "        \"\"\"\n",
    "        # initialize\n",
    "        self.initialize()\n",
    "        # model\n",
    "        self.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Write Proof (with proper notation) of the Compatible Function Approximation Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the following two conditions are satisfied:\n",
    "\n",
    "Critic gradient is compatible with the Actor score function\n",
    "$$\\nabla_w Q(s,a;w)= \\nabla_\\theta log\\pi_(a,s;\\theta)$$\n",
    "Critic parameters w minimize the following mean-squared error:\n",
    "$$\\epsilon=\\int_S \\rho^\\pi(s)\\int_A \\nabla_\\theta \\pi_(a,s;\\theta)(Q^\\pi(s,a)-Q(s,a;w)^2)da \\cdot ds $$\n",
    "Then the Policy Gradient using critic $Q(s,a;w)$ is exact:\n",
    "$$\\nabla_\\theta J(\\theta)=\\int_S \\rho^\\pi(s)\\int_A \\nabla_\\theta \\pi_(a,s;\\theta)Q(s,a;w)da\\cdot ds$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimize $\\epsilon$\n",
    "$$\\epsilon=\\int_S \\rho^\\pi(s)\\int_A \\pi_(a,s;\\theta)(Q^\\pi(s,a)-Q(s,a;w)^2)da \\cdot ds $$\n",
    "$$\\int_S \\rho^\\pi(s)\\int_A \\pi_(a,s;\\theta)(Q^\\pi(s,a)-Q(s,a;w)\\nabla_w Q(s,a;w))da \\cdot ds =0$$\n",
    "Since in condition 1 :\n",
    "$$\\nabla_w Q(s,a;w)= \\nabla_\\theta log\\pi_(a,s;\\theta)$$\n",
    "$$\\int_S \\rho^\\pi(s)\\int_A  \\pi_(a,s;\\theta)(Q^\\pi(s,a)-Q(s,a;w) \\nabla_\\theta log\\pi_(a,s;\\theta))da \\cdot ds =0$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\\int_S \\rho^\\pi(s)\\int_A  \\pi_(a,s;\\theta)Q^\\pi(s,a) \\nabla_\\theta log\\pi_(a,s;\\theta))da \\cdot ds \\\\\n",
    "=\\int_S \\rho^\\pi(s)\\int_A\\pi_(a,s;\\theta)Q(s,a;w) \\nabla_\\theta log\\pi_(a,s;\\theta))da \\cdot ds \\\\\n",
    "=\\int_S \\rho^\\pi(s)\\int_AQ(s,a;w) \\nabla_\\theta\\pi_(a,s;\\theta))da \\cdot ds $$\n",
    "\n",
    "Proved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means with conditions (1) and (2) of Compatible Function Approximation Theorem, we can use the critic func approx $Q(s,a;w)$ and still have the exact Policy Gradient."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
